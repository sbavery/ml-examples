# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_generic_ml.ipynb.

# %% auto 0
__all__ = ['check_gpu']

# %% ../nbs/00_generic_ml.ipynb 4
# General Utilities
import warnings
warnings.filterwarnings('ignore')
from pathlib import Path
import os
import wget
import glob
import shutil 
import tarfile

# Machine Learning
from fastai import *
from fastai.vision.all import *

# Autoreload modules before code execution
#%reload_ext autoreload
#%autoreload 2
# Plot inline within the notebook
#%matplotlib inline

# %% ../nbs/00_generic_ml.ipynb 5
## Utility Function to Check GPU Status
def check_gpu():
    print("CUDA Available: ", torch.cuda.is_available())
    num_devices = torch.cuda.device_count()
    if num_devices > 0:
        for device in range(0,num_devices):
            print("Device", device, "|", torch.cuda.get_device_name(device), 
            "| Allocated:", round(torch.cuda.memory_allocated(device)/1024**3,1), "GB",
            "| Cached:", round(torch.cuda.memory_reserved(device)/1024**3,1), "GB")

# %% ../nbs/00_generic_ml.ipynb 6
torch.cuda.empty_cache()
check_gpu()
